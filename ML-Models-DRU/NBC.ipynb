{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"NBC.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"}},"cells":[{"cell_type":"markdown","metadata":{"id":"lti3WORYZsmK"},"source":["# Naive Bayes Classifier\n","\n","Welcome to your next lab! You will build Naive Bayes Classifier.\n","\n","You will classify spam/ham messages.\n","\n","**You will learn to:**\n","- Build the general architecture of a learning algorithm with OOP in mind:\n","    - Helper functions\n","        - Preprocessing data\n","    - Main Model Class\n","        - Training\n","        - Prediction \n"]},{"cell_type":"markdown","metadata":{"id":"F86OV8dwZsmL"},"source":["## 1 - Packages ##\n","\n","First, let's run the cell below to import all the packages that you will need during this assignment.\n","- [numpy](www.numpy.org) is the fundamental package for scientific computing with Python.\n","- [pandas](https://pandas.pydata.org/) is a library providing a convenient work with data.\n","- [re](https://docs.python.org/3/library/re.html) is for regex"]},{"cell_type":"code","metadata":{"id":"eEoj1DOwZsmM","executionInfo":{"status":"ok","timestamp":1629969228023,"user_tz":-180,"elapsed":237,"user":{"displayName":"Bohdan Romanchuk","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjcca2-T6VFMBXztEA0IKKkYUy5dRi9uT1Evd-pnw=s64","userId":"08352315037258041530"}}},"source":["import pandas as pd\n","import numpy as np\n","import re"],"execution_count":2,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"b5Wy6bYpZsmO"},"source":["## 2 - Overview of the Problem set ##\n","\n","**Problem Statement**: You are given a dataset  containing:\n","    - a training set of m_train examples\n","    - a test set of m_test examples\n","    - each example is a message that belongs to a particular class: ham or spam.\n","\n","Let's get more familiar with the dataset. Load the data by running the following code.\n","\n","We won't divide our data to features(X) and target(Y) here, because we need to preprocess it in a special way."]},{"cell_type":"code","metadata":{"id":"DtUaewx1ZsmP","executionInfo":{"status":"ok","timestamp":1629969228282,"user_tz":-180,"elapsed":2,"user":{"displayName":"Bohdan Romanchuk","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjcca2-T6VFMBXztEA0IKKkYUy5dRi9uT1Evd-pnw=s64","userId":"08352315037258041530"}}},"source":["# Loading the data \n","\n","def load_data():\n","    df = pd.read_csv('spam.csv', encoding='latin-1')\n","    df_for_tests = df.head()\n","    \n","    idx = np.arange(df.shape[0])\n","    np.random.shuffle(idx)\n","\n","    train_set_size = int(df.shape[0] * 0.8)\n","\n","    train_set = df.loc[idx[:train_set_size]]\n","    test_set = df.loc[idx[train_set_size:]]\n","    \n","    return train_set, test_set, df_for_tests"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"ae4YTbLtZsmR","executionInfo":{"status":"ok","timestamp":1629969228619,"user_tz":-180,"elapsed":3,"user":{"displayName":"Bohdan Romanchuk","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjcca2-T6VFMBXztEA0IKKkYUy5dRi9uT1Evd-pnw=s64","userId":"08352315037258041530"}}},"source":["train_set, test_set, df_for_tests = load_data()"],"execution_count":4,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8fxIXA7FZsmU"},"source":["## 3 - Naive Bayes Classifier\n","**Mathematical expression of the algorithm**:\n","\n","\n","This algorithm is based on Bayes' theorem:\n","    $$ \\begin{equation}\n","    P(A_{j}\\hspace{0.1cm}\\rvert\\hspace{0.1cm}x_{1},\\dots,x_{n}) = \\frac{P(x_{1},\\dots,x_{n}\\hspace{0.1cm}\\rvert\\hspace{0.1cm}A_{j})P(A_{j})}{P(x_{1},\\dots,x_{n})} \n","    \\end{equation}$$\n","    \n","Ignoring denominator (because it stays the same for all cases):\n","\n","$$ \\begin{equation}\n","    P(A_{j})P(x_{1},\\dots,x_{n}\\hspace{0.1cm}\\rvert\\hspace{0.1cm}A_{j}) = P(A_{j}, x_{1},\\dots,x_{n}) = \\\\\n","  \\hspace{1cm} = P(x_{1}\\hspace{0.1cm}\\rvert\\hspace{0.1cm}x_{2},\\dots,x_{n}, A_{j})P(x_{2}\\hspace{0.1cm}\\rvert\\hspace{0.1cm}x_{3}, \\dots ,x_{n}, A_{j})\\dots P(x_{n-1}\\hspace{0.1cm}\\rvert\\hspace{0.1cm}x_{n}, A_{j}) \\approx \\\\\n","  \\hspace{1cm}\n","  \\end{equation}$$\n","By making an assumption that the $x_{i}$ are conditionally independent of each other:\n","$$ \\begin{equation} \\approx P(A_{j}) \\prod_{i=1}^{n} P(x_{i}\\hspace{0.1cm}\\rvert\\hspace{0.1cm}A_{j})\n","   \\end{equation}$$\n","   \n","We can calculate the probability, if we know the prior probability:\n","\n","$$ \\begin{equation}\n","    y^{*} = \\operatorname*{arg\\,max}_{j} \\big(P(A_{j}) \\prod_{i=1}^{n} P(x_{i}\\hspace{0.1cm}\\rvert\\hspace{0.1cm}A_{j})\\big)\n","   \\end{equation}$$\n","   \n","   \n","Due to floating point underflow, the above is usually replaced with the numerically tractable expression:\n","\n","$$ \\begin{equation}\n","    y^{*} = \\operatorname*{arg\\,max}_{j} \\big( \\ln(P(A_{j})) + \\sum_{i=1}^{n} \\ln(P(x_{i}\\hspace{0.1cm}\\rvert\\hspace{0.1cm}A_{j})) \\big)\n","   \\end{equation}$$\n","   \n","   \n","#### Laplace smoothing\n","\n","In statistics, additive smoothing, also called Laplace smoothing, or Lidstone smoothing, is a technique that is used to smooth categorical data. Given an observation \n","$\\begin{equation}\n","    x = (x_{1}\\, \\dots \\,x_{k})\n"," \\end{equation}$ from a multinomial distribution with N trials, a \"smoothed\" version of the data gives the estimator:\n","\n","$$ \\begin{equation}\n","    \\theta_i = \\frac{x_{i} + \\alpha}{N + \\alpha k}\n","   \\end{equation}$$\n","\n","where the pseudocount \n","$\\begin{equation}\n","    \\alpha > 0\n"," \\end{equation}$ is the smoothing parameter (\n","$\\begin{equation}\n","    \\alpha = 0\n"," \\end{equation}$ corresponds to no smoothing)."]},{"cell_type":"markdown","metadata":{"id":"G9mF4dTVZsmU"},"source":["### 3.1 - Preprocessing the data \n","\n","Our data consists of different messages. Messages contain some excess symbols, which don't affect the content of the text, but add noise to the data.\n","For example: \"Does not \\\\operate 66.7 after  & lt;# & gt;  or what\". \n","\n","Let's clean our data and leave only letters a-z, A-Z, numbers 0-9 and cast all letters to lowercase, replace double to n spaces with just one space, remove trailing spaces."]},{"cell_type":"code","metadata":{"id":"OZDyrOMZZsmV","executionInfo":{"status":"ok","timestamp":1629969231068,"user_tz":-180,"elapsed":238,"user":{"displayName":"Bohdan Romanchuk","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjcca2-T6VFMBXztEA0IKKkYUy5dRi9uT1Evd-pnw=s64","userId":"08352315037258041530"}}},"source":["# Clean the data\n","\n","def clean_data(message):\n","    \n","    \"\"\" \n","    Returns string which consists of message words\n","    \n","    Argument:\n","    message -- message from dataset; \n","        type(message) -> <class 'str'>\n","    \n","    Returns:\n","    result -- cleaned message, which contains only letters a-z, and numbers 0-9, with only one space between words;\n","        type(clean_data(message)) -> <class 'str'>\n","    \n","    \"\"\"\n","    \n","    ### START CODE HERE ###\n","    res = \"\"\n","    \n","    for c in message:\n","        if (c.isalnum() or c.isspace()):\n","            res += c\n","        else:\n","            res += \" \"\n","    \n","    return \" \".join(res.lower().split())\n","    ### END CODE HERE ###"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"r96K1BnlZsmX","executionInfo":{"status":"ok","timestamp":1629969232377,"user_tz":-180,"elapsed":247,"user":{"displayName":"Bohdan Romanchuk","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjcca2-T6VFMBXztEA0IKKkYUy5dRi9uT1Evd-pnw=s64","userId":"08352315037258041530"}},"outputId":"ee746cb7-3bd2-4714-f2ba-a812f69bdf14"},"source":["sentence = 'Doesn\\'t get, how{to}% \\\\operate+66.7 :after[it]\"\" & lt;# & gt; won\\'t `or(what)'\n","print('cleaned: ',clean_data(sentence))"],"execution_count":6,"outputs":[{"output_type":"stream","text":["cleaned:  doesn t get how to operate 66 7 after it lt gt won t or what\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"t-A6HAOtZsmY"},"source":["**Expected Output**: \n","\n","<table style=\"width:70%\">\n","    <tr>\n","        <td><b>cleaned:</b></td>\n","       <td> doesn t get how to operate 667 after it lt gt won t or what </td>\n","    </tr>\n","    \n","\n","</table>"]},{"cell_type":"markdown","metadata":{"id":"JsLYrKmtZsmZ"},"source":["Now let's clean each sentence and split data on features(X) and target(Y)"]},{"cell_type":"code","metadata":{"id":"gEFD2wuFZsma","executionInfo":{"status":"ok","timestamp":1629969233927,"user_tz":-180,"elapsed":243,"user":{"displayName":"Bohdan Romanchuk","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjcca2-T6VFMBXztEA0IKKkYUy5dRi9uT1Evd-pnw=s64","userId":"08352315037258041530"}}},"source":["# Preparation data for model\n","\n","def prep_for_model(train_set, test_set):\n","    \n","    \"\"\" \n","    Returns arrays of train/test features(words) and train/test targets(labels)\n","    \n","    Arguments:\n","    train_set -- train dataset, which consists of train messages and labels; \n","        type(train_set) -> pandas.core.frame.DataFrame\n","    test_set -- test dataset, which consists of test messages and labels; \n","        type(train_set) -> pandas.core.frame.DataFrame\n","    \n","    Returns:\n","    train_set_x -- array which contains lists of words of each cleaned train message; \n","        (type(train_set_x) ->numpy.ndarray[list[str]], train_set_x.shape = (num_messages,))\n","    train_set_y -- array of train labels (names of classes), \n","        (type(train_set_y) -> numpy.ndarray, train_set_y.shape = (num_messages,))\n","    test_set_x -- array which contains lists of words of each cleaned test message;\n","        (type(test_set_x) numpy.ndarray[list[str]], test_set_x.shape = (num_messages,)\n","    test_set_y -- array of test labels (names of classes), \n","        (type(test_set_y) -> numpy.ndarray, test_set_y.shape = (num_messages,))\n","    \n","    \"\"\"\n","    \n","    ### START CODE HERE ###\n","    train_num_messages = train_set.shape[0]\n","    test_num_messages = test_set.shape[0]\n","\n","    train_set_x = np.array(train_set['v2'].apply(clean_data).apply(str.split)).reshape((train_num_messages,))\n","    train_set_y = np.array(train_set['v1']).reshape((train_num_messages,))\n","    \n","    test_set_x = np.array(test_set['v2'].apply(clean_data).apply(str.split)).reshape((test_num_messages,))\n","    test_set_y = np.array(test_set['v1']).reshape((test_num_messages,))\n","    ### END CODE HERE ###\n","    \n","    return train_set_x, train_set_y, test_set_x, test_set_y\n","\n","train_set_x, train_set_y, test_set_x, test_set_y = prep_for_model(train_set, test_set)"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bnNLInEBZsmc","executionInfo":{"status":"ok","timestamp":1629969234786,"user_tz":-180,"elapsed":4,"user":{"displayName":"Bohdan Romanchuk","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjcca2-T6VFMBXztEA0IKKkYUy5dRi9uT1Evd-pnw=s64","userId":"08352315037258041530"}},"outputId":"15d20bd6-290a-4f44-ba12-6bd9c4996b7e"},"source":["a1, a2, b1, b2 = prep_for_model(df_for_tests.head(3), df_for_tests.tail(2))\n","print(a2[0], a1[0])\n","print(b2[0], b1[0])"],"execution_count":8,"outputs":[{"output_type":"stream","text":["ham ['go', 'until', 'jurong', 'point', 'crazy', 'available', 'only', 'in', 'bugis', 'n', 'great', 'world', 'la', 'e', 'buffet', 'cine', 'there', 'got', 'amore', 'wat']\n","ham ['u', 'dun', 'say', 'so', 'early', 'hor', 'u', 'c', 'already', 'then', 'say']\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Gm4MoiogZsmf"},"source":["**Expected Output**: \n","\n","<table style=\"width:40%\">\n","    <tr>\n","        <td><b>ham:</b></td>\n","       <td> ['go', 'until', 'jurong', 'point', 'crazy', 'available', 'only', 'in', 'bugis', 'n', 'great', 'world', 'la', 'e', 'buffet', 'cine', 'there', 'got', 'amore', 'wat'] </td>\n","    </tr>\n","    <tr>\n","        <td><b>ham:</b></td>\n","       <td> ['u', 'dun', 'say', 'so', 'early', 'hor', 'u', 'c', 'already', 'then', 'say']\n"," </td>\n","    </tr>\n","\n","</table>"]},{"cell_type":"markdown","metadata":{"id":"Hn_Ag9ZQZsmg"},"source":["Now let's check words in each category"]},{"cell_type":"code","metadata":{"id":"olXobIliZsmg","executionInfo":{"status":"ok","timestamp":1629969237183,"user_tz":-180,"elapsed":4,"user":{"displayName":"Bohdan Romanchuk","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjcca2-T6VFMBXztEA0IKKkYUy5dRi9uT1Evd-pnw=s64","userId":"08352315037258041530"}}},"source":["# Check words in categories\n","\n","def categories_words(x_train, y_train):\n","    \n","    \"\"\"\n","    Returns arrays of features(words) in each category and in both categories\n","    \n","    Arguments:\n","    x_train -- array which contains lists of words of each cleaned train message; \n","        (type(x_train) -> numpy.ndarray[list[str]], x_train.shape = (num_messages,))\n","    \n","    Returns:\n","    all_words_list -- array of all words in both categories;\n","        (type(all_words_list) -> numpy.ndarray[str], all_words_list.shape = (num_words,))\n","    ham_words_list -- array of words in 'ham' class;\n","        (type(ham_words_list) -> numpy.ndarray[str], ham_words_list.shape = (num_words,))\n","    spam_words_list -- array of words in 'spam' class;\n","        (type(spam_words_list) -> numpy.ndarray[str], spam_words_list.shape = (num_words,))        \n","    \"\"\"\n","    all_words_list = []\n","    ham_words_list = []\n","    spam_words_list = []\n","    \n","    ### START CODE HERE ###\n","    for i in range(len(x_train)):\n","        all_words_list += x_train[i]\n","        if (y_train[i] == 'ham'):\n","            ham_words_list += x_train[i]\n","        else:\n","            spam_words_list += x_train[i]\n","    \n","    num_all = len(all_words_list)\n","    num_ham = len(ham_words_list)\n","    num_spam = len(spam_words_list)\n","    \n","    all_words_list = np.array(all_words_list).reshape((num_all,))\n","    ham_words_list = np.array(ham_words_list).reshape((num_ham,))\n","    spam_words_list = np.array(spam_words_list).reshape((num_spam,))\n","    ### END CODE HERE ###\n","    \n","    return all_words_list, ham_words_list, spam_words_list\n","\n","all_words_list_a1, ham_words_list_a1, spam_words_list_a1 = categories_words(a1, a2)"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zCkJB-7TZsmj","executionInfo":{"status":"ok","timestamp":1629969237901,"user_tz":-180,"elapsed":4,"user":{"displayName":"Bohdan Romanchuk","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjcca2-T6VFMBXztEA0IKKkYUy5dRi9uT1Evd-pnw=s64","userId":"08352315037258041530"}},"outputId":"185dfa0b-c2c5-4bfb-b8f3-ae6bde5e75c6"},"source":["print('first five \"ham\" words of a1: ', ham_words_list_a1[:5])"],"execution_count":10,"outputs":[{"output_type":"stream","text":["first five \"ham\" words of a1:  ['go' 'until' 'jurong' 'point' 'crazy']\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"phUVohsyZsmk"},"source":["**Expected Output**: \n","\n","<table style=\"width:40%\">\n","    <tr>\n","        <td><b>first five \"ham\" words of a1:</b></td>\n","       <td> ['go' 'until' 'jurong' 'point' 'crazy'] </td>\n","    </tr>\n","\n","</table>"]},{"cell_type":"markdown","metadata":{"id":"AARnBVXGZsmk"},"source":["### 3.2 Model"]},{"cell_type":"code","metadata":{"id":"LO7_8FEKZsmm","executionInfo":{"status":"ok","timestamp":1629969757210,"user_tz":-180,"elapsed":230,"user":{"displayName":"Bohdan Romanchuk","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjcca2-T6VFMBXztEA0IKKkYUy5dRi9uT1Evd-pnw=s64","userId":"08352315037258041530"}}},"source":["class Naive_Bayes(object):\n","    \"\"\"\n","    Parameters:\n","    -----------\n","    alpha: int\n","        The smoothing coeficient.\n","    \"\"\"\n","    def __init__(self, alpha):\n","        self.alpha = alpha\n","        \n","        self.train_set_x = None\n","        self.train_set_y = None\n","        \n","        self.all_words_list = []\n","        self.ham_words_list = []\n","        self.spam_words_list = []\n","    \n","    def fit(self, train_set_x, train_set_y):\n","        \n","        # Generate all_words_list, ham_words_list, spam_words_list using function 'categories_words'; \n","        # Calculate probability of each word in both categories\n","        ### START CODE HERE ### \n","        self.all_words_list, self.ham_words_list, self.spam_words_list = categories_words(train_set_x, train_set_y)\n","        \n","        (all_unique, all_counts) = np.unique(self.all_words_list, return_counts=True)\n","        (ham_unique, ham_counts) = np.unique(self.ham_words_list, return_counts=True)\n","        (spam_unique, spam_counts) = np.unique(self.spam_words_list, return_counts=True)\n","        \n","        ham_probs = (ham_counts + self.alpha) / np.sum(all_counts)\n","        spam_probs = (spam_counts + self.alpha) / np.sum(all_counts)\n","\n","        self.default_prob = 1 / (np.sum(all_counts) + self.alpha * len(self.all_words_list))\n","        \n","        self.prob_of_ham = np.count_nonzero(train_set_y == \"ham\") / len(train_set_y)\n","        self.prob_of_spam = np.count_nonzero(train_set_y == \"spam\") / len(train_set_y)\n","\n","        self.words_probs = {\n","            \"ham_words\": ham_unique,\n","            \"ham_probs\": ham_probs,\n","\n","            \"spam_words\": spam_unique,\n","            \"spam_probs\": spam_probs\n","        }\n","        ### END CODE HERE ### \n","        \n","    def predict(self, test_set_x):\n","        \n","        # Return list of predicted labels for test set; type(prediction) -> list, len(prediction) = len(test_set_y)\n","        ### START CODE HERE ###\n","        prediction = np.array(['spam'] * len(test_set_x))\n","\n","        for i in range(len(test_set_x)):\n","            p_ham = np.log(self.prob_of_ham)\n","            p_spam = np.log(self.prob_of_spam)\n","            (unique, counts) = np.unique(test_set_x[i], return_counts=True)\n","            for j in range(len(unique)):\n","                if (unique[j] in self.words_probs[\"ham_words\"]):\n","                    p_ham += np.log(counts[j] * self.words_probs[\"ham_probs\"][self.words_probs[\"ham_words\"] == unique[j]], dtype='float')\n","                else:\n","                    p_ham += np.log(self.default_prob)\n","                if (unique[j] in self.words_probs[\"spam_words\"]):\n","                    p_spam += np.log(counts[j] * self.words_probs[\"spam_probs\"][self.words_probs[\"spam_words\"] == unique[j]], dtype='float')\n","                else:\n","                    p_spam += np.log(self.default_prob)\n","            if (p_spam < p_ham):\n","                prediction[i] = 'ham'\n","            \n","        ### END CODE HERE ### \n","        return prediction"],"execution_count":11,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QVU0yzP1Zsmo"},"source":["## 4 - Training"]},{"cell_type":"markdown","metadata":{"id":"KIiilw0KZsmo"},"source":["First of all, we should define a smoothing coeficient (`alpha`)."]},{"cell_type":"code","metadata":{"id":"two70VtTZsmp","executionInfo":{"status":"ok","timestamp":1629969758828,"user_tz":-180,"elapsed":3,"user":{"displayName":"Bohdan Romanchuk","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjcca2-T6VFMBXztEA0IKKkYUy5dRi9uT1Evd-pnw=s64","userId":"08352315037258041530"}}},"source":["a = 1"],"execution_count":12,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"F0FVH2nMZsmr"},"source":["Now we can initialize our model:"]},{"cell_type":"code","metadata":{"id":"lFOgwc0eZsmr","executionInfo":{"status":"ok","timestamp":1629969759511,"user_tz":-180,"elapsed":2,"user":{"displayName":"Bohdan Romanchuk","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjcca2-T6VFMBXztEA0IKKkYUy5dRi9uT1Evd-pnw=s64","userId":"08352315037258041530"}}},"source":["model = Naive_Bayes(alpha=a)"],"execution_count":13,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DBs3q36OZsmu"},"source":["Let's train our model:"]},{"cell_type":"code","metadata":{"id":"dh3_xodbZsmv","executionInfo":{"status":"ok","timestamp":1629969759997,"user_tz":-180,"elapsed":265,"user":{"displayName":"Bohdan Romanchuk","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjcca2-T6VFMBXztEA0IKKkYUy5dRi9uT1Evd-pnw=s64","userId":"08352315037258041530"}}},"source":["model.fit(train_set_x, train_set_y)"],"execution_count":14,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MruvACv2Zsmw"},"source":["## 5 - Making predictions"]},{"cell_type":"code","metadata":{"id":"0-zUCJU_Zsmy","executionInfo":{"status":"ok","timestamp":1629969764508,"user_tz":-180,"elapsed":3594,"user":{"displayName":"Bohdan Romanchuk","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjcca2-T6VFMBXztEA0IKKkYUy5dRi9uT1Evd-pnw=s64","userId":"08352315037258041530"}}},"source":["y_predictions = model.predict(test_set_x)"],"execution_count":15,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"B8SVxG6MxObs","executionInfo":{"status":"ok","timestamp":1629969857026,"user_tz":-180,"elapsed":225,"user":{"displayName":"Bohdan Romanchuk","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjcca2-T6VFMBXztEA0IKKkYUy5dRi9uT1Evd-pnw=s64","userId":"08352315037258041530"}},"outputId":"a189de2b-cfc2-4850-c5d5-70b748976f7c"},"source":["np.unique(y_predictions, return_counts=True)"],"execution_count":18,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(array(['ham', 'spam'], dtype='<U4'), array([1019,   96]))"]},"metadata":{},"execution_count":18}]},{"cell_type":"markdown","metadata":{"id":"FNA1wZRAZsmy"},"source":["Let's calculate accuracy (accuracy of model must be >0.95):"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KBFDdUdkZsm0","executionInfo":{"status":"ok","timestamp":1620581188780,"user_tz":-180,"elapsed":733,"user":{"displayName":"Bohdan Romanchuk","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjcca2-T6VFMBXztEA0IKKkYUy5dRi9uT1Evd-pnw=s64","userId":"08352315037258041530"}},"outputId":"7314e909-c160-45de-ace9-f55707d44ce9"},"source":["actual = list(test_set_y)\n","accuracy = (y_predictions == test_set_y).mean()\n","print(accuracy)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["0.9551569506726457\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"7-Wk8eTRZsm1"},"source":["## 6 - Conclusion\n","As we can see, our model fits well the hypothesis function to the data.\n","\n","#### What's next:\n","1. Try experimenting with the `alpha` to see how this affects the model you have built.\n","2. Compare the results you have obtained with the `sklearn.naive_bayes.MultinomialNB` model.\n","3. Try this model in the wild! Select your favorite dataset [here](https://www.kaggle.com/datasets?sortBy=hottest&group=public&page=1&pageSize=20&size=small&filetype=all&license=all&tagids=13303) and play with it."]}]}